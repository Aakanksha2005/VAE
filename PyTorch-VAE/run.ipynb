# %%
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
from collections import OrderedDict
from models import VanillaVAE  # Import your trained VAE model

# Ensure plots display inside the notebook
%matplotlib inline  

# Load the trained VAE model
checkpoint_path = "logs/VanillaVAE/version_7/checkpoints/last.ckpt"  # Update if needed
vae = VanillaVAE(3, 128)

# Load checkpoint and fix state_dict keys
checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))
new_state_dict = OrderedDict()
for key, value in checkpoint["state_dict"].items():
    new_key = key.replace("model.", "")  # Remove "model." prefix
    new_state_dict[new_key] = value
vae.load_state_dict(new_state_dict)
vae.eval()

# Load and preprocess an image
image_path = "/home/juhitharadha/Data/PyTorch-VAE/Data/celeba/img_align_celeba/000103.jpg"  # Change this to your image path
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

image = Image.open(image_path).convert("RGB")
input_tensor = transform(image).unsqueeze(0)  # Add batch dimension

# Pass the image through the VAE
with torch.no_grad():
     outputs = vae(input_tensor)
     print(len(outputs))  # Check how many values are returned

     reconstructed, _,_,_  = vae(input_tensor)

# Convert tensor to numpy for visualization
reconstructed_image = reconstructed.squeeze(0).permute(1, 2, 0).numpy()
reconstructed_image = (reconstructed_image - reconstructed_image.min()) / (reconstructed_image.max() - reconstructed_image.min())  # Normalize

# Plot original and reconstructed images
fig, ax = plt.subplots(1, 2, figsize=(8, 4))
ax[0].imshow(image)
ax[0].set_title("Original Image")
ax[0].axis("off")

ax[1].imshow(reconstructed_image)
ax[1].set_title("Reconstructed Image")
ax[1].axis("off")

plt.show()



# %% [markdown]
# 

# %%
import os
import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from torchvision.datasets import ImageFolder
from torchvision.utils import save_image
from PIL import Image
from torch.utils.data import DataLoader, Subset
from models import VanillaVAE
from collections import OrderedDict
from pytorch_fid import fid_score
from tqdm import tqdm

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load VAE
vae = VanillaVAE(3, 128).to(device)
checkpoint = torch.load("logs/VanillaVAE/version_7/checkpoints/last.ckpt", map_location=device)
new_state_dict = OrderedDict({k.replace("model.", ""): v for k, v in checkpoint["state_dict"].items()})
vae.load_state_dict(new_state_dict)
vae.eval()

# Data transform
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

# Load 10k images from CelebA
data_dir = "/home/juhitharadha/Data/PyTorch-VAE/Data/celeba/img_align_celeba"
dataset = ImageFolder(root=os.path.dirname(data_dir), transform=transform)
test_subset = Subset(dataset, range(10000))
test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)

# Directory to save real and generated images
os.makedirs("fid/real", exist_ok=True)
os.makedirs("fid/fake", exist_ok=True)

# Save 10k real images
for i, (img, _) in enumerate(tqdm(test_loader, desc="Saving real images")):
    for j in range(img.size(0)):
        if i * 64 + j >= 10000:
            break
        save_image(img[j], f"fid/real/img_{i * 64 + j}.png")

# Generate and save 10k synthetic images
with torch.no_grad():
    for i in tqdm(range(10000), desc="Generating synthetic images"):
        z = torch.randn((1, 128)).to(device)  # Generate latent vectors
        gen_img = vae.decode(z)# Pass device
        save_image(gen_img, f"fid/fake/img_{i}.png")

# Calculate FID
fid = fid_score.calculate_fid_given_paths(["fid/real", "fid/fake"], batch_size=64, device=str(device),dims = 2048)
print("FID Score:", fid)



# %%
import os
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader, Subset
from models import VanillaVAE
from collections import OrderedDict
from pytorch_fid import fid_score
from tqdm import tqdm
import torch.nn.functional as F
from torchvision.utils import save_image


# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load trained VAE (adjust input channels and latent dim accordingly)
vae = VanillaVAE(1, 128).to(device)  # MNIST has 1 channel
checkpoint = torch.load("logs/VanillaVAE/version_16/checkpoints/last.ckpt", map_location=device)
new_state_dict = OrderedDict({k.replace("model.", ""): v for k, v in checkpoint["state_dict"].items()})
vae.load_state_dict(new_state_dict)
vae.eval()

# Data transform for MNIST
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor()
])

# Load MNIST test set (10,000 images)
mnist_test = torchvision.datasets.MNIST(root="./MNIST", train=False, download=True, transform=transform)
test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)

# Prepare FID directories
os.makedirs("fid_mnist/real", exist_ok=True)
os.makedirs("fid_mnist/fake", exist_ok=True)

# Save real MNIST images
for i, (img, _) in enumerate(tqdm(test_loader, desc="Saving real MNIST images")):
    for j in range(img.size(0)):
        idx = i * 64 + j
        if idx >= 10000:
            break
        save_image(img[j], f"fid_mnist/real/img_{idx}.png")

# Generate and save synthetic MNIST images
with torch.no_grad():
    for i in tqdm(range(10000), desc="Generating synthetic MNIST images"):
        z = torch.randn((1, 128)).to(device)
        gen_img = vae.decode(z) # This will show the shape of the generated image tensor
        gen_img = F.interpolate(gen_img, size=(28, 28), mode='bilinear', align_corners=False)
        gen_img = gen_img.mean(dim=1, keepdim=True)
        save_image(gen_img, f"fid_mnist/fake/img_{i}.png")

# Compute FID score
fid = fid_score.calculate_fid_given_paths(["fid_mnist/real", "fid_mnist/fake"], batch_size=64, device=str(device), dims=2048)
print("FID Score (MNIST):", fid)



# %%
import os
import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from models import InfoVAE  # Make sure this is your InfoVAE class
from collections import OrderedDict
from pytorch_fid import fid_score
from tqdm import tqdm
import torch.nn.functional as F

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load trained InfoVAE
vae = InfoVAE(latent_dim=128,in_channels = 1).to(device)  # Set the appropriate latent dimension for InfoVAE
checkpoint = torch.load("logs/InfoVAE/version_4/checkpoints/last.ckpt", map_location=device)
new_state_dict = OrderedDict({k.replace("model.", ""): v for k, v in checkpoint["state_dict"].items()})
vae.load_state_dict(new_state_dict)
vae.eval()

# Data transform for MNIST
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor()
])

# Load MNIST test set (10,000 images)
mnist_test = torchvision.datasets.MNIST(root="./MNIST", train=False, download=True, transform=transform)
test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)

# Prepare FID directories
os.makedirs("fid_mnist/real", exist_ok=True)
os.makedirs("fid_mnist/fake", exist_ok=True)

# Save real MNIST images
for i, (img, _) in enumerate(tqdm(test_loader, desc="Saving real MNIST images")):
    for j in range(img.size(0)):
        idx = i * 64 + j
        if idx >= 10000:
            break
        save_image(img[j], f"fid_mnist/real/img_{idx}.png")

fid = fid_score.calculate_fid_given_paths(["fid_mnist/real", "fid_mnist/fake"], batch_size=64, device=str(device), dims=2048)
print("FID Score (MNIST):", fid)

# Reconstruction Loss (InfoVAE typically uses ELBO)


# %%
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torchvision.models import inception_v3
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torch.nn.functional import adaptive_avg_pool2d
from PIL import Image
from scipy.linalg import sqrtm
import numpy as np

class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings=512, embedding_dim=20, beta=0.25):
        super(VectorQuantizer, self).__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.beta = beta

        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)

    def forward(self, z_e):
        z_e_flat = z_e.view(-1, self.embedding_dim)
        
        # Compute squared Euclidean distance manually
        distances = (z_e_flat ** 2).sum(dim=1, keepdim=True) \
                    + (self.embedding.weight ** 2).sum(dim=1) \
                    - 2 * torch.matmul(z_e_flat, self.embedding.weight.T)
        
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        z_q = self.embedding(encoding_indices).view_as(z_e)
        
        commitment_loss = F.mse_loss(z_e, z_q.detach())
        codebook_loss = F.mse_loss(z_q, z_e.detach())
        loss = commitment_loss + self.beta * codebook_loss
        
        z_q = z_e + (z_q - z_e).detach()
        return z_q, loss

class VQVAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20, num_embeddings=512):
        super(VQVAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        self.vq = VectorQuantizer(num_embeddings, latent_dim)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z_e = self.encoder(x)
        z_q, vq_loss = self.vq(z_e)
        recon_x = self.decoder(z_q)
        return recon_x, z_q, z_e, vq_loss

def vqvae_loss(recon_x, x, vq_loss):
    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='mean')
    return recon_loss + vq_loss

# Data preparation
transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])
train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True),
                          batch_size=128, shuffle=True)
test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True),
                         batch_size=16, shuffle=True)

# Model and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vqvae = VQVAE().to(device)
optimizer = optim.Adam(vqvae.parameters(), lr=1e-3)

# Training loop
num_epochs = 4
torch.autograd.set_detect_anomaly(True)  # Detect NaN and Inf values
vqvae.train()
for epoch in range(num_epochs):
    total_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, z_q, z_e, vq_loss = vqvae(data)
        loss = vqvae_loss(recon_batch, data, vq_loss)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.4f}")

# Generate reconstructions
vqvae.eval()

# Pass the image through the VAE


with torch.no_grad():
    test_data, _ = next(iter(test_loader))
    test_data = test_data.to(device)
    reconstructions, _, _, _ = vqvae(test_data)
    test_data = test_data.view(-1, 1, 28, 28).cpu()
    reconstructions = reconstructions.view(-1, 1, 28, 28).cpu()

# Display images
num_images = 8
fig, axes = plt.subplots(num_images, 2, figsize=(6, 12))
for i in range(num_images):
    axes[i, 0].imshow(test_data[i].squeeze(), cmap="gray")
    axes[i, 0].set_title("Original")
    axes[i, 0].axis("off")
    axes[i, 1].imshow(reconstructions[i].squeeze(), cmap="gray")
    axes[i, 1].set_title("Reconstructed")
    axes[i, 1].axis("off")
plt.tight_layout()
plt.show()
with torch.no_grad():
    random_latents = torch.randn(10000, 20).to(device)  # Sample random latent vectors
    z_q, _ = vqvae.vq(random_latents)
    new_samples = vqvae.decoder(z_q).view(-1, 1, 28, 28).cpu()
    
# Save the generated images in a folder named "VQVAE_gen" and test images as "VQVAE_TEST"
import os
os.makedirs("VQVAE_gen", exist_ok=True)
os.makedirs("VQVAE_TEST", exist_ok=True)
for i in range(1000):
    img = new_samples[i].squeeze().cpu().numpy() * 255
    img = Image.fromarray(img.astype(np.uint8), mode='L')
    img.save(f"VQVAE_gen/sample_{i}.png")
for i in range(min(1000, len(test_data))):  # Ensure the loop does not exceed the size of test_data
    img = test_data[i].squeeze().cpu().numpy() * 255
    img = Image.fromarray(img.astype(np.uint8), mode='L')
    img.save(f"VQVAE_TEST/sample_{i}.png")

# Display new samples
fig, axes = plt.subplots(4, 4, figsize=(6, 6))
for i, ax in enumerate(axes.flat):
    ax.imshow(new_samples[i].squeeze(), cmap="gray")
    ax.axis("off")
plt.tight_layout()
plt.show()
# --- Evaluation and visualization ---

vqvae.eval()
num_images = 16

with torch.no_grad():
    test_data, _ = next(iter(test_loader))  # Fetch once for consistent pairs
    test_data = test_data.to(device)
    reconstructions, _, _, _ = vqvae(test_data)

    test_data = test_data.view(-1, 1, 28, 28).cpu()
    reconstructions = reconstructions.view(-1, 1, 28, 28).cpu()

# Display two separate 4x4 grids
def show_grid(images, title):
    fig, axes = plt.subplots(4, 4, figsize=(5, 5))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i].squeeze(), cmap="gray")
        ax.axis("off")
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

show_grid(test_data[:num_images], "Original Images")
show_grid(reconstructions[:num_images], "Reconstructed Images")


def get_activations(images, model, device):
    model.eval()
    activations = []

    preprocess = transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # Convert 1-channel to 3-channel
    ])

    with torch.no_grad():
        for img in images:
            if isinstance(img, np.ndarray):
                img = Image.fromarray((img * 255).astype(np.uint8))
            img = preprocess(img).unsqueeze(0).to(device)
            pred = model(img)  # logits and features
            if pred.dim() == 4:  # Ensure the tensor has 4 dimensions
                pred = adaptive_avg_pool2d(pred, output_size=(1, 1))
                pred = pred.view(pred.size(0), -1)  # Flatten the tensor to (batch_size, features)
            elif pred.dim() == 2:  # Handle already flattened tensors
                pass
            else:
                raise ValueError("Unexpected tensor dimensions. Ensure input to the model is correct.")
            activations.append(pred.squeeze().cpu().numpy())
    
    return np.array(activations)

def calculate_fid(act1, act2):
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)

    diff = mu1 - mu2
    covmean = sqrtm(sigma1 @ sigma2)

    # Numerical instability fix
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)
    return fid

# Usage:
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inception = inception_v3(pretrained=True, transform_input=False).to(device)
inception.fc = torch.nn.Identity()  # remove the final classification layer

# Use test_data as real_images and new_samples as generated_images
real_images = [img.squeeze().cpu().numpy() for img in test_data]
generated_images = [img.squeeze().cpu().numpy() for img in new_samples]

real_acts = get_activations(real_images, inception, device)
fake_acts = get_activations(generated_images, inception, device)
fid_score = calculate_fid(real_acts, fake_acts)

print(f"FID Score: {fid_score}")


